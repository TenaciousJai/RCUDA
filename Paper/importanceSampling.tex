\subsection{Importance Sampling}\label{subsec:IS}

Importance sampling is a standard approach that allows for the numerical
approximation of expected values that cannot be computed directly. 
Suppose we want to compute $E_{f}[h(X)]$ where $f$ denotes the 
probability density (or mass) function of the random variable X i.e., 
\begin{align}\label{eq:IS}
 E_{f}[h(X)] = \int h(x) f(x) dx .
\end{align}
If we are able to sample independent random variates $x_{1},\ldots,x_{n}$
from $f$ then we can approximate the expectation in~\eqref{eq:IS} in a
straightforward manner using:
\begin{align}\label{eq:ISbasicapprox}
  E_{f}[h(X)] = \frac{1}{n} \sum_{i=1}^{n} h(x_i) .
\end{align}
Now suppose it is difficult or computationally intensive to sample from
the density $f$. Instead, we can sample $x_{1},\ldots,x_{n}$ from another
density $g$ with the same support as $f$. By a simple importance weighting
scheme we can modify the approximation in~\eqref{eq:ISbasicapprox} to obtain
\begin{align}\label{eq:ISapprox}
  E_{f}[h(X)] = \frac{1}{n} \sum_{i=1}^{n} w(x_{i})h(x_{i}) ,
\end{align}
where $w(x_i) = f(x_i)/g(x_i)$ is the importance weight given to $x_{i}$.

We now apply this technique to a simple example from Mathew Shum
(\url{www.hss.caltech.edu/~mshum/gradio/simulation.pdf}).
Suppose we want to compute the expectation of a truncated standard 
Normal with support $[0,1]$. Note that the density of the truncated Normal is
$$f(x) = \frac{\phi(x)}{\int_{0}^{1} \phi(x)} dx$$ 
where $\phi(x)$ is the standard normal density. Instead of sampling
from $f(x)$, we instead sample $N$ variates from a standard Uniform
i.e., we select $g(x) = 1_{\left\{0<x<1\right\}}$. Since $g(x)=1$ the
importance weights in~\eqref{eq:ISapprox} can just be seen to 
be $w(x_{i})=\phi(x_i)/.34134$. This can be easily implemented in \R{} using
\begin{RCode}
N = 1e6
x = runif(N)
mean(x * dnorm(x)/.34134)
\end{RCode}
%This yields $0.46$.

Like the \Rfunc{dnorm} example, we could also implement this
vectorized computation in \R{} as a computation on a GPU
with each thread calculating $x_i w(x_i)$.
\begin{CCode}
extern "C"
__global__ void truncNorm(float *out, float *unifVals, int N)
{
    int myblock = blockIdx.x + blockIdx.y * gridDim.x;
           /* how big is each block within a grid */
    int blocksize = blockDim.x * blockDim.y * blockDim.z;
            /* get thread within a block */
    int subthread = threadIdx.z*(blockDim.x * blockDim.y) + 
                     threadIdx.y*blockDim.x + threadIdx.x;

    float phi0_1 = 0.3413447460685;
    int idx = myblock * blocksize + subthread;
    if(idx < N) {
       out[idx] = unifVals[idx] * dnorm(unifVals[idx], 0, 1)/phi_0_1;
    }
}
\end{CCode}
As is typical with a GPU kernel, the code determines
which part of the data this particular instance of the kernel is to
work on. It does this using its threadIdx and blockIdx and 
the grid and block dimensions.
The actual computations are very simple
\begin{CCode}
out[idx] = unifVals[idx] * dnorm(unifVals[idx], 0, 1)/phi_0_1;
\end{CCode}
We omit the code for the \Cfunc{dnorm} routine since it is very
similar to that in the previous example. Full code is 
available at \url{http://www.omegahat.org/RCUDA/importanceSample.cu},
along with some additional kernels.

We compile this code and load it with
\begin{RCode}
nvcc('importanceSampling.cu')
mod = loadModule('importanceSampling.ptx')
u = runif(N)
z = .gpu(mod$truncNorm, numeric(N), u, as.integer(N), 
         gridDim = c(64, 32), blockDim = 512, outputs = 1L)
\end{RCode}
%$
Here we have determined that we can run 512 threads in each block
and so determine that we need \Rexpr{ceiling(N/512)} blocks to compute
all \Rvar{N}  values.

Do we gain much from using the GPU in this circumstance?
We have had to write the kernel in \C{} and compile it.
Undoubtedly, this is more complicated than the \R{} code above
and involves debugging, etc.
What we do get is the parallelism. 
Table~\ref{tab:ImportanceSamplingTimings} shows timings
on different GPUs for different length vectors.
\begin{table}
\begin{tabular}{lrr}
 N  & Quadro 600 & Tesla K20 \\
1e6 &  1.16  &  \\
1e7 &  1.61  &  \\
\end{tabular}
\caption{\textbf{Speedup for simple importance sample.}
These are relative speedups of the GPU implementation relative
to the vectorized \R{} implementation.
The Quadro 600 GPU has $1$ gigabyte of RAM and is more of a
graphics card than a GPGPU.
The Tesla K20 has 5 gigabytes of RAM and is one of the top-of-the-line
GPUs at present.
}\label{tab:ImportanceSamplingTimings} 
\end{table}
\begin{comment}
ids = lapply(list.files(pattern = "rda$"), load, globalenv())
times = structure(lapply(unlist(ids), function(id) get(id)[3,]), names = unlist(ids))
tm = cbind(data.frame(times = sapply(times, median)), as.data.frame(do.call(rbind, strsplit(unlist(ids), "\\."))))
names(tm) = c('times', 'fun', 'N', 'GPU')
with(tm, tapply(tm, list(N, GPU), function(x) x[1, 'times']/x[2, 'times']))
\end{comment}
%$



We note that we have elected to compute the random values from the
uniform density in \R{} and explicitly pass these to the kernel.
Alternatively, we could use the CUDA random number generators.
This avoids transferring data from \R{} to the GPU memory.
It also allows us to compute these values in parallel. Examples
involving the generation of random numbers on the GPU using
the CURAND library~\citep{bib:curand} are also 
available in the \texttt{examples} directory of \Rpkg{RCUDA}. 

