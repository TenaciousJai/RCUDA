\documentclass[article]{jss}
\usepackage{comment}
\usepackage{graphicx}
%\usepackage{tikz}
%\usetikzlibrary{shadows,trees}
\usepackage{fancyvrb}
\usepackage{cprotect}
\usepackage{color}
\usepackage{amsmath}
%\usepackage[firstpage]{draftwatermark}
%\SetWatermarkLightness{.95}

\newcommand{\note}[1]{\textbf{\textcolor{red}{#1}}}
\def\Rtrue{\textbf{TRUE}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Paul Baines\\University of California at Davis\\
        Duncan Temple Lang\\University of California at Davis}
\title{RCUDA: General programming facilities for GPUs in R}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Paul Baines, Duncan Temple Lang}
\Plaintitle{RCUDA: General programming facilities for GPUs in R}
\Shorttitle{RCUDA}

%% an abstract and keywords
\Abstract{ General Purpose Graphical Programming Units (GPGPUs)
  provide the ability to perform computations in a massively parallel
  manner.  Their potential to significantly speed computations for
  certain classes of problems has been clearly demonstrated.  We
  describe an \proglang{R} package that provides high-level
  functionality that allows \proglang{R} programmers to experiment
  with and exploit NVIDIA GPUs.  The package provides an interface to
  the routines and data structures in the CUDA (Compute Unified Device
  Architecture) software development kit (SDK), and also provides
  higher-level \proglang{R} interfaces to these. Currently,
  programmers write the code that runs on the GPU in \proglang{C} code
  but call that code and manage the inputs and outputs in \proglang{R}
  code. We describe experimental approaches to compile \proglang{R}
  directly so that all computations can be expressed in \proglang{R}.
} \Keywords{GPUs, parallel computing, \R, \Rpkg{RCUDA} package}
\Plainkeywords{GPUs, parallel computing, R, RCUDA} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Paul Baines\\
  4210 Math Sciences Building, \\
  University of California at Davis \\
  One Shields Avenue\\
  Davis, CA 95616\\
  E-mail: \email{pbaines@ucdavis.edu}\\
  URL: \url{http://www.stat.ucdavis.edu/~pdbaines/}
\\
\\
  Duncan Temple Lang\\
  4210 Math Sciences Building, \\
  University of California at Davis \\
  One Shields Avenue\\
  Davis, CA 95616\\
  E-mail: \email{duncan@r-project.org}\\
  URL: \url{http://www.omegahat.org}
}
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\usepackage[strings]{underscore}
%\usepackage[T1]{fontenc}
%\catcode`\_=12

%\usepackage{newtxtext,newtxmath}

\def\CUDA{CUDA}
\def\C{\proglang{C}}
\def\R{\proglang{R}}
\def\llvm{\proglang{LLVM}}
\def\Rpkg#1{\pkg{#1}}
\def\Rfunc#1{\textsl{#1()}}
\def\Rop#1{\texttt{#1}}
\def\Rvar#1{\textsl{#1}}
\def\Cfunc#1{\textit{#1()}}
\def\Cvar#1{\textit{#1}}
\def\Cparam#1{\textsl{#1}}
\def\Cnull{\textsl{NULL}}
\def\Rnull{\texttt{NULL}}
\def\file#1{\textbf{#1}}
\def\Ctype#1{\texttt{#1}}
\def\Rclass#1{\textit{#1}}
\def\Rarg#1{\textbf{#1}}
\def\Roption#1{\dquote{\textsl{#1}}}

\def\Rexpr#1{\texttt{#1}}

\def\dquote#1{``#1''}

\def\ShFlag#1{\textit{#1}}

\DefineVerbatimEnvironment{RCode}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CCode}{Verbatim}{fontshape=tt}
\DefineVerbatimEnvironment{ShCode}{Verbatim}{fontshape=it}
\DefineVerbatimEnvironment{ROutput}{Verbatim}{fontshape=bf}

\begin{document}

\section{Introduction}\label{sec:Introduction}
In recent years Graphics Processing Units (GPUs) have emerged as the 
dominant computational platform for massively  parallel computation. 
Fortunately, unlike CPUs, the performance of GPUs continues to improve
dramatically from year to year. While the high-throughput, memory-light
paradigm of GPU programming is not well-suited to all problems, it is
well-suited to particular classes of computations, some quite common in
statistics. As the size of both modern data sets and modern computational
requirements continue to grow it is increasingly important for statisticians
to be able to leverage the power of GPUs to conduct these computations in 
an efficient manner.

% Compute Unified Device Architecture

In presenting the \pkg{RCUDA} package we assume the reader has a basic
familiarity with GPUs and the NVIDIA-specific Compute Unified Device
Architecture (CUDA) programming model. For an introduction to GPU
programming and CUDA see~\cite{bib:pmpp}. As we will see in later sections
\pkg{RCUDA} provides a high-level interface to the CUDA SDK and lessens
the burden for \R{} programmers seeking to utilize the power of GPUs.
Nonetheless, GPUs are tailored for massively parallel computation and
perform best when the programmer understands the fundamentally different
computational model required by GPUs.
Most general purpose programming on GPUs (GPGPU) is built upon a slightly extended
version of the  \C{}  programming language (e.g., CUDA, OpenCL) and requires
the programmer to think about two processing units - the host CPU
and the GPU device. When coding in CUDA C the programmer must explicitly 
allocate and move data from the host to the device and back. This memory 
management and data transfer introduces a programming burden as well as a 
performance cost. As we will see in section~\ref{subsec:memory}, \pkg{RCUDA} 
has the ability to automatically handle much of the programming burden. 
The performance costs of data transfer to/from the device are problem specific
and are an important consideration when designing and optimizing GPU code.

The \pkg{RCUDA} package is quite different in intent and functionality
than other GPU-related \R\, packages such as
\Rpkg{gputools}~\citep{bib:gputools} and \Rpkg{rgpu}~\citep{bib:rgpu}.
\Rpkg{gputools} implements several commonly used statistical
algorithms in \C{} code that runs on a GPU and provides \R{} functions
to invoke those with data in \R.  The set of functions is fixed and
\R{} programmers wanting to implement a different algorithm or
approach for one of these algorithms must program in \C.

The \Rpkg{rgpu} package also provides implementation of a few
algorithms written in \C{} that run on a GPU.  However, it also
provides an ``interpreter'' for \R{} scalar mathematical expressions.
This does not appear to handle arbitrary code % in spite of its claims
and also has to map each expression from \R{} to a different
representation for each call from \R{} to the GPU.
It also interprets this on the GPU rather than using native code. 
% It  also doesn't install cleanly.  
We discuss in section~\ref{sec:LLVM} how might be able to compile a
larger subset of the \R{} language to native GPU code.

The \Rpkg{RCUDA} package is similar in nature to
\Rpkg{OpenCL}~\citep{bib:ROpenCl}. Whereas \Rpkg{OpenCL} can 
in principal be used with all GPUs, \Rpkg{RCUDA} specifically targets
the CUDA SDK and NVIDIA GPUs. This is based on both the belief that 
the dedicated CUDA SDK can outperform the more general OpenCL
programming model~\citep{bib:OpenCL} and the wider popularity of CUDA in the GPGPU 
community. In addition, \Rpkg{RCUDA} aims to expose the entire SDK to \R{} 
programmers. The
\Rpkg{OpenCL} package provides the essential functionality to invoke
kernels on the GPU, passing data from \R{} to the GPU and back.  This
difference illustrates one of the primary motivations of the
\Rpkg{RCUDA} package. We want \R{} programmers to experiment with
different features of the SDK and to explore the performance
characteristics of various programming strategies for GPUs.  Different
GPUs exhibit quite different performance characteristics, and
different programming paradigms and even tuning parameters also can
have significant impact. For this reason, we want to be able to be
able to control these aspects dynamically in a high-level language 
rather than fix them statically in a language such as \C.

%Norm Matloff's thrust package.


\section[The Basics of the RCUDA Package]{The Basics of the \Rpkg{RCUDA} package}\label{sec:basics}
In this section, we describe both the essential concepts of the CUDA
SDK and the \R{} interface to it provided by the \Rpkg{RCUDA} package.

At its simplest, computing with GPUs involves 
\begin{enumerate}
\item writing (and compiling) a kernel that performs the computations,
\item allocating memory on the GPU device, 
\item copying data from the host to the device
\item invoking the kernel
\item copying the results back from the device to the host.
\end{enumerate}
We now discuss each of these steps and how they are handled by \Rpkg{RCUDA}.

The most fundamantal component of any GPGPU code is the kernel. 
At present we recommend that the kernel be compiled outside of \R.  This
means that an \R{} programmer can write and compile a kernel or
perhaps that it be compiled by somebody else and made available to the
\R{} programmer.  The \Rpkg{RCUDA} package can compile a kernel for
the \R{} user but it is typically more convenient to use the command-line
to compile the code directly. We now use the \Cfunc{dnorm} kernel as an example.
The code is reasonably simple for a CUDA kernel and is shown in
figure~\ref{fig:dnormKernelCode}. Again, we assume the reader has some
familiarity with CUDA kernels, and include the code in figure~\ref{fig:dnormKernelCode}
for illustration only. For readers who are less familiar with CUDA C, 
the key parts of the code are the \texttt{\_\_global\_\_} 
qualifier (which indicates that this is indeed a kernel), the 
thread/block indexing (handled using standard CUDA conventions) and the
actual calculation of the normal density. As written, the kernel is designed 
to be executed over a specfied number of threads as chosen by the 
programmer. The indexing portion of the code ensures that each 
thread operates on a unique element of the input vector (or does nothing
if the index is out of bounds). 
 % should we show the code for the kernel?
% This probably requires a separate section  describing the basic idea
% of kernels and how they  know what to operate on and where to put
% their results. threadIdx, blockIdx, blockDim, gridDim. This requires
% some explanation,
\begin{figure}
\begin{CCode}
extern "C"
__global__ void dnorm_kernel(float *vals, int N, float mu, float sigma)
{
       // Taken from geco.mines.edu/workshop/aug2010/slides/fri/cuda1.pd
    int myblock = blockIdx.x + blockIdx.y * gridDim.x;
           /* how big is each block within a grid */
    int blocksize = blockDim.x * blockDim.y * blockDim.z;
            /* get thread within a block */
    int subthread = threadIdx.z*(blockDim.x * blockDim.y) + 
                         threadIdx.y*blockDim.x + threadIdx.x;

    int idx = myblock * blocksize + subthread;

	if(idx < N) {
            float std = (vals[idx] - mu)/sigma;
	    float e = exp( - 0.5 * std * std);
	    vals[idx] = e / ( sigma * sqrt(2 * 3.141592653589793));
	}
}
\end{CCode}
\caption{The \C{} code defining a GPU kernel to compute the Normal
  density.  This kernel writes the results back into the input
  vector, overwriting its values.
}\label{fig:dnormKernelCode}
\end{figure}

Before we can invoke a kernel, we have to load it into the host
process, i.e. \R.  We do this by loading the compiled code as a
\textit{Module}.  We use the \R{} function \Rfunc{loadModule} to do
this. It can read the code in various formats - the PTX text format
and the two binary formats cubin and fatbin.

The code can be compiled at the command line using 
\begin{ShCode}
nvcc --ptx -o dnorm.ptx dnorm.cu
\end{ShCode}
Alternatively, we can compile the two binary formats with 
\begin{ShCode}
nvcc -cubin -m64 -gencode arch=compute_10,code=sm_10 -o dnorm.cubin dnorm.cu
nvcc -fatbin -m64 -gencode arch=compute_10,code=sm_10 -o dnorm.fatbin dnorm.cu
\end{ShCode}
respectively. Note that the \ShFlag{arch} flag specifies the GPU compatability level. %! Fix this
Once we have compiled the code, we can load it with
\begin{RCode}
filename = system.file("sampleKernels", "dnorm.ptx", package = "RCUDA")
mod = loadModule(filename)
\end{RCode}
Next, we can obtain a reference to the particular routine in the
module  that we want to invoke.  The \R{} interface makes the module
appear like a list and so we can use
\begin{RCode}
kernel = mod$dnorm_kernel
\end{RCode}
%$ - fool latex mode
to get this reference.

We now have the kernel, so before executing it we need the data to pass to it. Here for simplicity 
we just simulate the data in \R{} via the \Rfunc{rnorm} function.
\begin{RCode}
N = 1e6
mean = 2.3
sd =  2.1
x = rnorm(N, mean, sd)
\end{RCode}
As outlined at the beginning of section~\ref{sec:basics}, memory needs
to be allocated on the device, and the data copied from host to device 
before invoking the kernel. However, using the \R{} interface to the GPU
provided by \Rpkg{RCUDA} we can simply call the kernel with host data 
using the \Rfunc{.cuda} (or the synonomous \Rfunc{.gpu}) and the 
\R{} interface will allocate memory and copy the \R{} vector
argument to the device without user input. In short, we can invoke the 
kernel with
\begin{RCode}
ans = .cuda(kernel, x, N, mean, sd, gridDim = c(62, 32), blockDim = 512)
\end{RCode}
Note that we must explicitly specify the dimension for the grid and block to be
used when launching the kernel. Both arguments can take positive integer 
inputs of up to three-dimensions with any omitted dimensions defaulting to $1$.
We chose $62$, $32$ and $512$ for this example as the product
of these exceeds the number of elements \Rvar{N}. Since the number of
threads exceeds the number of elements to be operated on, there will be
some redundant threads on the GPU (as determined by the \texttt{if}
statement in figure~\ref{fig:dnormKernelCode}). Since thread launches
are exceptionally cheap on GPUs, this over-saturation strategy is 
standard practice for GPU programming.

The \Rfunc{.cuda} function recognizes the inputs and determines which
to copy to the device and which can be passed directly (i.e. the
scalar values \Rvar{N}, \Rvar{mean} and \Rvar{sd}). As we will see later
we can pass arguments that refer to data or memory already on the
GPU and \Rfunc{.cuda} recognizes that it doesn't need to transfer
this, but merely pass the reference as-is. The option of using 
either host or device resident arguments in the call to \Rfunc{.cuda}
provides flexibility and simplicity while retaining the ability to
produce highly efficient code that minimizes data transfers between
the host and device.

%Q: does the following stuff apply for matrix/array inputs?

When calling the \Rfunc{.cuda} function with an \R{} vector as an input argument,
\Rfunc{.cuda} recognizes that since the data was passed from \R{}, it 
may be modified by the kernel and thus returns the vector.  If
there are multiple vector inputs, \Rfunc{.cuda} returns all of these as
a list in the same style as the \Rfunc{.C} function. However, \Rfunc{.cuda}
also recognizes if only one input argument is a vector, and thus \Rvar{ans} in
our example also contains the actual vector of normal density values.  More
generally, we can specify which inputs are to be copied back to \R{}
from the device via the \Rarg{outputs} parameter of the \Rfunc{.cuda}
function.

We conclude this simple example by noting that \Rfunc{.cuda} executes the kernel calls using
the current context which we describe in section~\ref{subsec:Contexts}. 
This same context must be used to both load the module and call any of its kernels.

\subsection{Manually Allocating Memory on the Device}\label{subsec:memory}
While the \Rfunc{.cuda} function processes (most) \R{} vectors for us,
we may want to explicitly control how values are passed from the
host (CPU) to the device (GPU). The \Rpkg{RCUDA} package provides
functions to control this and also some short-hand, covenient
mechanisms for implementing the transfers.  The two fundamental
functions are \Rfunc{copyToDevice} and \Rfunc{copyFromDevice}.  These
are reasonably flexible functions.  \Rfunc{copyToDevice} takes an \R{}
object and copies its contents to memory on the GPU device.  By
default, it allocates the space on the device, using information about
the number of elements and the type of each element in the \R{} object
to determine the space needed.  For example, we can copy an \R{}
vector to the GPU in the following manner:
\begin{RCode}
dev.ptr = copyToDevice(x)
\end{RCode}

We can also explicitly allocate space on the device and then copy
values to that space.  The function \Rfunc{cudaMalloc} (and its alias
\Rfunc{cudaAlloc}) allocates space on the device.  We pass it the
number of elements and either the size of each element (in bytes) or
the name of the element type which it looks up to determine the number
of bytes for each element.  We can explicitly allocate memory and pass
this as the destination target for \Rfunc{copyToDevice}, e.g.
\begin{RCode}
ptr = cudaAlloc(N, elType = "numeric")
copyToDevice(x, ptr)
\end{RCode}

% Duncan -- think you meant non-parametric bootstrap here?

Why would we want to explicitly allocate space on the device rather
then letting \R{} copy vectors for us?  The most common situation in
which this is desirable is when we want to allocate space on the 
device once and then reuse it to store different values at different
times. For example, consider a simple non-parametric bootstrap in which we
generate many samples of the same length by sampling with replacement
and apply some kernel to each generated dataset. In this context we can
reuse the same memory for each bootstrap dataset. In the code below we 
demonstrate how to allocate the space once, copy the boostrap data to it
each iteration and then compute the necessary summaries via a call
to a GPU kernel, e.g.,
\begin{RCode}
ptr = cudaMalloc(length(x), elType = class(x))
replicate(B, {
      copyToDevice(sample(x), ptr)
      .cuda(kernel, ptr, N, mean, sd, gridDim = c(62, 32), blockDim = 512))
})
\end{RCode}

By default, \Rfunc{copyToDevice} uses \Rfunc{cudaMalloc} to allocate
the space on the device.  \Rfunc{cudaMalloc} returns an object that
points to the allocated memory, but also contains the number and the
type of the elements, if specified.  This allows us to retrieve the
contents of the memory on the device.  The \Rfunc{copyFromDevice}
function allows for explicitly copying data from the device to host.
It takes the pointer, the number of elements and
the type of each element. While we can specify these arguments explicitly,
in many cases it is more convenient to use the subset operator
\begin{RCode}
p[]
\end{RCode}
This utilizes the information stored when we allocated the space and
is equivalent to 
\begin{RCode}
copyFromDevice(p, p@nels, p@elTypeName)
\end{RCode}

We can also use the subset syntax to assign values to an existing
memory location on the device. For example, 
\begin{RCode}
p[] <- rnorm(N)
\end{RCode}
copies the the values on the right-hand side to the device memory 
specified by \texttt{p[]}.

In addition to this standard usage it is also worth noting that we can
use \Rfunc{cudaMalloc} to allocate space for arbitrary data
types since we only need the size of each element. For known data types
such as \Ctype{float} or \Ctype{int} values, \Rpkg{RCUDA} knows how to
copy data both to and from the device. For more general data types
this can be done by the \R{} programmer using the functionality 
provided by \Rpkg{RCUDA} 

When we no longer have an \R{} reference memory on the device (i.e. in
an \R{} variable), \R{} releases the memory using a finalizer routine
we register when allocating the space.  This allows \R{} programmers
to ignore memory management issues and treat the memory on the device
as a regular \R{} object.


%Pinned memory and other functions in the API.

\subsection{Contexts}\label{subsec:Contexts}
We can call many of the functions in the \Rpkg{RCUDA}  package
without having to know about CUDA context objects.
However, they are important for some computations and so we 
address them briefly here.

Most \Rpkg{RCUDA} functions require an active context.  We can
explicitly create a default context using \Rfunc{createContext} or we
can use \Rfunc{cuGetContext} to query the current context and create
one if there is none.  When creating a context, we can specify the
device with which it is associated and also specify different flags or
options to control how it behaves.  These options are a combination of
individual options which we can specify in different ways. 
We can use \R{} variables representing the different options, e.g.
\Rvar{CU_CTX\_SCHED\_AUTO} and \Rvar{CU\_CTX\_MAP\_HOST},
and then we can combine them with the \Rop{|} operator. 
Alternatively, we can use a vector of names that identify the
different options. 
The following are equivalent
\begin{RCode}
c("SCHED_AUTO", "BLOCKING_SYNC", "MAP_HOST")
c("CU_CTX_SCHED_AUTO", "CU_CTX_BLOCKING_SYNC", "CU_CTX_MAP_HOST")
CU_CTX_SCHED_AUTO | CU_CTX_BLOCKING_SYNC | CU_CTX_MAP_HOST
\end{RCode}
Note that the first variant avoids the \texttt{"CU_CTX"} prefix.
Similarly, there are also functions that can query and set attributes of a
context such as the stack and heap size, shared memory configuration
and cache configuration.

CUDA maintains a stack of contexts (per host thread, of which there
is only one in \R).  When we create a context, it becomes the active
one and is used in subsequent computations.  We can also explicitly
push an existing context on to the top of the stack with
\Rfunc{cuCtxPushCurrent}.  We can pop the current context off the top
of the stack with \Rfunc{cuCtxPopCurrent}.  At present, \R{} users
must explicitly release a context with \Rfunc{cuCtxtDestroy}.  It
would be preferable to release the memory using \R's finalizer
mechanism. However, this is, at best, complex because it is difficult
to determine if CUDA is still using the context.


\subsection{Querying Devices}
The function \Rfunc{getNumDevices} tells us the number of devices on
the local machine.  
We can get the name of a device with \Rfunc{cuDeviceGetName}, e.g.
\begin{RCode}
cuDeviceGetName(1L)
\end{RCode}
\begin{ROutput}
[1] "GeForce GT 330M"
\end{ROutput}
We can query the characteristics of a device using
the \Rfunc{getDeviceProperties} function or
\Rfunc{cuDeviceGetAttributes}.  The former uses a deprecated routine
in the CUDA API, while the second queries all of the individual
attributes.  The queryable attributes include the maximum dimensions
of a grid and a block, the maximum number of threads per block and per
processor, the warp size, the number of multi-processors and the shared
memory per block.  The names of the entire set of attributes are
available  via the \Rvar{CUdevice\_attributeValues} variable in the package.
Rather than querying all of the attributes in one call,
we can retrieve a single attribute with \Rfunc{cuDeviceGetAttribute}.
We specify the attribute using the name or value of one 
of the elements in \Rvar{CUdevice\_attributeValues}.


In addition to querying information about a device,
we can query the version of the CUDA SDK with
\Rfunc{cudaVersion} e.g.,
\begin{RCode}
cudaVersion()
\end{RCode}
\begin{ROutput}
 driver runtime 
   5000    5000 
\end{ROutput}


We can determine the amount of memory a device has with the function
\Rfunc{cuMemInfo}. Unlike the other functions for querying a device,
\Rfunc{cuMemInfo} does not take a device as an argument. Instead, it uses the device
associated with the current context.  Thus we must have created a
context before calling this function, either explicitly or implicitly.
The function reports the total memory on the device, the amount free
and the proportion free.


\subsection{Profiling}
The primary reason to use GPUs for scientific computing is usually to improve
performance. Key considerations in the performance of GPU code include 
the overhead of copying data between the host and the device, the 
efficiency of the kernel code and even the
dimensions of the grid and block for controlling the threads.
In light of this, it is
useful to understand where the entire code spends time in order to
improve the performance by reducing these bottlenecks.  As in \R{}
itself, we can profile computations involving the GPU with several
routines in the CUDA API.

% Duncan -- I removed the product from the bootstrap example below, 
% and made it more general. It now computes summary statistics over
% bootstrapped datasets rather than computing the likelihood
% for each bootstrapped data set (I couldn't see why you would
% want to do this in practice -- maybe I am missing something though).

We can profile an entire sequence of \R{} expressions with the \R{}
function \Rfunc{profileCUDA}.  This takes one or more \R{} expressions
(enclosed within \texttt{\{\}} for more than one expression) and returns the
profiling information as a data frame.
The following bootstrap example allocates space and reuses it for each
bootstrapped data set, with GPU code used to compute summary statistics 
for each sample:  
\begin{RCode}
B = 100
prof = profileCUDA( {
  p = copyToDevice(x)
  replicate(B, {
      p[] = sample(x, N, replace = TRUE)
      .cuda(kernel, p, N, mean, sd, outputs = 1,  
                   gridDim = c(62, 32), blockDim = 512)
  })
})
\end{RCode}
This returns a data frame with one row for each line of output generated
by the CUDA profiler.  Each row corresponds to a particular CUDA
routine, kernel routine or routine called by the kernel.  There are
typically multiple rows for the same routine corresponding to when the
routine was observed by the profiler.  The columns of the data frame
correspond to the \dquote{counters} we specified for the profiler to
record.  We specify this via a configuration file the profiler reads
when it is instantiated. The \Rpkg{RCUDA} package provides a default
configuration file and uses that if the caller does not specify one.
We can use a different configuration file either by explicitly passing
a file name in the call to \Rfunc{profileCUDA} via the \Rarg{config}
parameter, or globally via the \R{} option
\Roption{CUDAProfilerConfig}.


Rather than profiling an entire collection of \R{} expressions as a
single unit, we can create a profiler and start and stop it at
different times to collect information about particular \R{}
expressions.  The \Rfunc{cudaProfiler} function creates the profiler,
and optionally starts it.  We specify the name of a file to which the
profiler writes the information, and we can select either of two
formats - CSV or name=value pairs.  We start and stop the profiler
with \Rfunc{cudaStartProfiler} and \Rfunc{cudaStopProfiler}.  We can
read the CSV output from the profiler with \Rfunc{readCUDAProfile}.
Note that \Rfunc{profileCUDA} is merely a wrapper that uses all of these
functions.

The \Rfunc{summary} method for the result of \Rfunc{profileCUDA} and
\Rfunc{readCUDAProfile} aggregates the rows in the data frame for each
routine and gives the total counts for each as a new data frame.

\subsection{Memory Management}
As discussed in section~\ref{sec:basics}, the \Rpkg{RCUDA} package provides
both high- and low-level functionality to transfer data between the host
and the GPU device. In this section we describe some of the richer
memory management functionality offered by the package.
As may be expected, the low-level functionality mirrors the CUDA 
API with the ability to
allocate memory on the device in several different ways, copy memory
between the host and device, and to release the memory.  These include
\Rfunc{cudaMalloc} and \Rfunc{cudaMallocPitch} for allocating memory
and \Rfunc{cudaMemcpy} for copying the contents of memory between the
two devices.  \Rfunc{cuMemFree} releases the memory.
We can use these primitives directly or alternatively
we can use higher-level functionality provided by the package.

In a call to \Rfunc{.gpu} (or \Rfunc{.cuda}), any \R{} vector with
more than one element is automatically transferred from \R{} to the
GPU.  By default, after the kernel execution has completed, the
contents of this memory on the GPU are then transferred back to \R{}
and returned by the \Rfunc{.gpu} call.  The memory is then automatically
released. This is done via a finalizer on the external pointer.
Typically if a vector argument does not contain any useful output data from the
kernel then we would not want \Rfunc{.gpu} to spend time transfering
its contents back from the GPU. Fortunately we can avoid this by 
specifying which arguments are to be considered outputs from the kernel
via the \Rarg{outputs} paramater of \Rfunc{.gpu}.

% This seemed a little repetitive (think it was all covered in earlier section)
%
% By default, this allocates space on the device
% for us, determining the number of bytes from the length of the \R{}
% object and the size of each element, mapped to GPU types.  We can
% specify this target element type.  We can also allocate the space
% ourselves with \Rfunc{cudaMalloc} and pass this explicitly as the
% value of the \Rarg{to} parameter.  This allows us to allocate space

As discussed in section~\ref{sec:basics}, rather than have \Rfunc{.gpu}
implicitly transfer data to the device, we can also explicitly control
the transfer of data from \R{} to the GPU with \Rfunc{copyToDevice}.
Manually copying data from host to device is often preferable if
we are applying one or more kernels to different \R{} vectors or matrices
with the same number of elements. The \Rfunc{copyToDevice} function 
returns an object that is an instance
of the class \Rfunc{cudaPtrWithLength}.  These objects have the
address of the memory on the GPU in which the data are
stored. However, they also contain the number of elements and the size
of each element on the GPU.  This allows us to retrieve all of the
values in the array from the GPU as the object is fully
self-describing (unlike a simple pointer).
We can use the simple empty-subsetting operator to copy the
contents from the GPU back to \R. For example, the following
shows how to copy an \R{} vector to the GPU and retrieve it:
\begin{RCode}
x = rnorm(1e6)
ptr = copyToDevice(x)
ptr[]
\end{RCode}
The final expression retrieves the collections values from the GPU as
into a new \R{} vector, separate from the original vector in \Rvar{x}.
We can also a subset of the  elements in the usual manner, e.g.
\begin{RCode}
ptr[1:10]
ptr[which(x) > 0 ]
ptr[ - c(11:1e6)]
\end{RCode}
% Verify these are doing what we think they are.

We can explicitly free memory we allocate on the GPU with
\Rfunc{cudaMalloc} via the function \Rfunc{cuMemFree}.  However, \R{}
will also do this for us via the usual garbage collection.  When an \R{}
object referencing memory on the GPU is deleted, the finalizer routine
calls \Cfunc{cuMemFree} for us.  We have to remember to remove such
variables when we no longer need them as otherwise the memory will not
be released.  We also have to wait until the \R{} garbage collector is
invoked for the GPU memory to actually be freed.  This is why the
\Rfunc{.gpu} function explicitly calls \Rfunc{gc}, by default. If it
did not, it is possible that memory on the GPU used from a previous
call to \Rfunc{.gpu} would still appear to be in use and so not
released. This could have lead to situations where there was 
insufficient memory on the GPU to perform this new call to \Rfunc{.gpu},
when in practice we could have used the memory from the previous call.

In addition to the \Rfunc{cudaMalloc} function, there
are other functions available for memory allocation and data transfer to/from
the GPU such as \Rfunc{cudaMallocPitch}, \Rfunc{cudaMemcpy2D} 
and \Rfunc{cudaMemcpy3D} which allow for the transfer of 
2-dimensional and 3-dimensional arrays. Further discussion
of some aspects of this functionality is provided in~\ref{subsec:distances}.

\subsection{Asynchronous execution}
A typical mode of using the GPU is to launch a kernel and wait for it
to complete and then return the results to the \R{} calling command.
We can, however, launch the kernel asynchronously.  This allows us to
dispatch one or more tasks to the GPU while we continue to perform
tasks in the \R{} session.  This allows us to truly use the GPU as a
co-processor.

There are several ways to excute tasks asynchronously.  The simplest
is, perhaps, to use the \Rarg{.async} parameter in the \Rfunc{.gpu}
function.  If this is \Rtrue, the \Rfunc{.gpu} function launches the
kernel and does not wait for it it to complete.  It returns the
references to memory on the GPU device for any objects it copied to
the GPU.  At this point, we can evaluate other \R{} expressions while
the GPU is processing the kernel threads.  To obtain the results from
the kernel, we can simply copy the values from the GPU memory.
Copying the memory from the device will cause CUDA to synchronize with
the GPU.  Alternatively, we can explicitly synchronize with the GPU by
calling \Rfunc{cudaDeviceSynchronize} or \Rfunc{cuCtxSynchronize}.

A different approach than using \Rfunc{.async} is to use streams.  We
can think of a stream as managing a collection of tasks. A task can be
an invocation of a kernel or copying data to or from the GPU.  We can
add a kernel invocation to a stream by passing the stream object to
the \Rfunc{.gpu} function.
We first create the stream with
\begin{RCode}
stream = cudaStreamCreate()
\end{RCode}
We then launch a kernel with
\begin{RCode}
out = .gpu(mod$kernel, ..., stream = stream)
\end{RCode}
%$
By default, the presence of the \Rarg{stream} argument
will cause \Rarg{.async} to be \Rtrue and 
\Rfunc{.gpu} will return without waiting for the kernel
to complete.

We can queue multiple kernel invocations on the same stream via
repeated calls to \Rfunc{.gpu}.  The GPU can interleave and schedule
the threads across these kernels in a way that improves the overall
performance relative to serial scheduling.

We can also add requests to a stream to copy memory to or from the
GPU.  For example, the \Rfunc{cudaMemcpyAsync} accepts a \Rarg{stream}
argument, as do any of the Async variants of the memory copying
functions.

While we can synchronize on the device or context, we can also use
more fine-grained synchronization using a stream. The
\Rfunc{cudaStreamSynchronize} function will wait in \R{} until the
entire collection of tasks in the stream have completed.  Rather than
blocking until the tasks are complete, we can use
\Rfunc{cudaStreamQuery} to query whether the stream's task are all
finished.  This allows us to periodically ``check in'' with stream
without actually waiting for it to complete.


%\note{TODO: Mention how this is done (if it is implemented yet).}

\subsection{Other Routines in the API}
% Briefly mention the general groupings of other routines, e.g. stream,
% event  and what we omitted

For the most part, there is a corresponding \R{} function for 
each routine in the CUDA SDK that can run on the host. At present 
we have not created bindings to the routines associated with textures
and surfaces. However, should these prove useful for the community, 
these bindings can be generated in a similar manner.



\input{Examples}



\section{Low-level interface and its implementation}
We initially implemented the bindings to copy data between the host
(CPU) and device (GPU) and the ability to launch a kernel to verify
that this approach would work and to deal with the marshalling of \R{}
objects to the kernel calls and vice-versa.  We also added some
additional bindings to query the available GPUs and their properties.
Subsequently, however, we programmatically generated the binding (\R{}
and \C) code and the enumerated constants and information about the
data structures (e.g. the size of each so we can use that when
allocating memory on the device).  

% Show an example of a binding and how it appears.
Next, we describe a reasonably simple example of a programmatically
generated binding to illustrate the nature of the interface and to
help mapping  from the CUDA documentation to the \R{} functions.
One function is \Rfunc{cuDeviceGetAttribute} and corresponds to the routine
with the same name in the CUDA API.
The routine is defined  in the header file \file{cuda.h} as
\begin{Code}
CUresult CUDAAPI 
cuDeviceGetAttribute(int *pi, CUdevice_attribute attrib, CUdevice dev);
\end{Code}
The second parameter identifies which attribute we are querying and
the third parameter identifies the device. The latter is an integer,
with $0$ corresponding to the first device, $1$ for the second and so
on.  The first parameter is a pointer to a single integer. This is how
the CUDA API returns the result of the query.  This is an output
variable. Accordingly, we do not need to include in the \R{} function
that calls the \C{} routine.

The CUDA routine returns a status value of type \Ctype{CUresult}.  We
check this to see if the call was successful. If it was, we return the
result stored in \Cvar{pi}; otherwise, we raise an error in \R, using
the particular value of the status type and the CUDA error message.

The \R{} function we generate to interface to the CUDA routine is
defined as
\begin{RCode}
cuDeviceGetAttribute <-
function( attrib , dev  )
{
   ans = .Call('R_auto_cuDeviceGetAttribute', 
                   as(attrib, 'CUdevice_attribute'), 
                   as(dev, 'CUdevice'))
   if(is(ans, 'CUresult') && ans != 0)
       raiseError(ans, 'R_auto_cuDeviceGetAttribute')
   ans
}
\end{RCode}
This makes the call to the wrapper routine
\Cfunc{R\_auto\_cuDeviceGetAttribute}, passing it the two \R{} values
identifying the attribute and device of interest.  An important step
here is that this \R{} function coerces the arguments it receives to
the correct types.  Here we have an opportunity to use $1$-based
counting familiar to \R{} users for specifying the device.  The
coercion from the device number to a \Rclass{CUdevice} object in \R{}
performs the subtraction to map to the $0$-based couting used by CUDA.
We manually defined the \Rclass{CUdevice} class and the coercion method.

The function checks the status of the result to see if it is an \R{}
object of class \Rclass{CUresult}. If it is and not $0$, the call
produced an error and we raise this in \R. By doing this in \R{}
rather than in \C{} code, it is easier to raise exceptions/conditions
with classes corresponding to the type of error. This allows
programmers to react to particular types of errors, e.g. out of memory
or invalid device, in different ways.

The final piece of the bindings is the \C{} wrapper routine
\Cfunc{R\_auto\_cuDeviceGetAttribute}. This is defined as 
\begin{CCode}
SEXP
R_auto_cuDeviceGetAttribute(SEXP r_attrib, SEXP r_dev)
{
    SEXP r_ans = R_NilValue;
    int pi;
    CUdevice_attribute attrib = (CUdevice_attribute) INTEGER(r_attrib)[0];
    CUdevice dev = INTEGER(r_dev)[0];
    CUresult ans;
    ans = cuDeviceGetAttribute(& pi,  attrib,  dev);
    if(ans)
       return(R_cudaErrorInfo(ans));
    r_ans = ScalarInteger(pi) ;
    return(r_ans);
}
\end{CCode}
The logic is quite straightforward and very similar to how we would
write this manually. We convert the \R-level arguments to their
equivalent \C{} types.  We create a local variable (\Cvar{pi}) used to
retrieve the actual result.  We call the CUDA routine and check the
status value.  If there is an error, we call a manually written
routine \Cfunc{R\_cudaErrorInfo} which collects information about the
CUDA error in the form of an \R{} object.  If the CUDA call was
successful, we return the value of the local variable \Cvar{pi}
converted to the approriate \R{} type.



We used \Rpkg{RCIndex}~\cite{bib:RCIndex} to both read the
declarations in the CUDA header files and to generate the bindings.
Programmatically generating the bindings reduces the number of simple
programming errors and also allows us to update the bindings when new
versions of the API and SDK are released.

The CUDA API uses an idiom for most of its routines.  A routine
returns an error status value, and if this indicates success, the
actual results are accessible via the pointers we passed to the
routine. In other words, the routines return their values via input
pointers.  We specialized the general binding generation mechanism in
\Rpkg{RCIndex} to exploit this idiom and be able to understand what
was essentially an output parameter passed as a pointer input and what
were actual pointer input parameters.  This code to generate the
CUDA-specific bindings is included in the package.

We also generated some of the documentation for the \R{} functions
programmatically.  The CUDA header files have documentation in
comments directly before each routine and data structure.  We used
\Rpkg{RCIndex} to extract these comments and then process the
different parts describing the purpose of the routine, its parameters
and return value.



\section[Compiling R code to native GPU code]{Compiling \R{} code to native GPU code}\label{sec:LLVM}
% Move into Future work if we don't get this running.
In this section we describe experimental work designed to 
allow programmers to utilize the power of the GPU without
ever needing to leave \R{} i.e., to allow the creation of CUDA
kernels directly from \R{} code. The approach to achieve this
is to use \llvm{} (Low Level Virtual Machine), a compiler toolkit 
that can be embedded within \R. 
This allows us to dynamically generate machine code within an \R{}
session and invoke those newly created routines, passing them 
inputs from \R{} and obtaining the results in \R.
This simple approach is very powerful and allows us to ``compile around the \R{}
interpreter'' and gain significant speedups for some common \R{}
idioms~\cite{bib:StatSciLLVM}.

\llvm{} can generate machine code for the machine's CPU, but can also
create PTX code to run on an NVIDIA GPU.  We can use our \R{} compiler
code (\Rpkg{RLLVMCompile}), or a specialized version of it for
targetting GPUs, and have it generate PTX code for kernel routines.
We can then load these routines onto the GPU and invoke them as a
regular kernels.  This was done using \R{} as an example by
researchers in NVIDIA~\cite{bib:libNVVM} investigating dynamic, interpreted
languages and GPUs.

\section{Future Work}

Now that we have the general-purpose bindings and access to the CUDA
API, we, and hopefully others, will explore how to map commonly used
statistical algorithms onto the GPU to take advantage of their potential.  
We plan to explore different examples and approaches and understand their
characteristics in the context of GPU applications.  

We also plan to explore the potential benefits of aspects of the API
not addressed in this paper such as pinned memory, streams for
interleaving computations and peer-to-peer communication between 
two or more GPUs.

As the need arises, we may generate bindings to other GPU-related
libraries and APIs. It is useful to note though that there is no need to have \R{}
bindings to interface with kernel-level libraries such as
Thrust~\cite{bib:Thrust} and parts of curand~\cite{bib:curand} as 
these can be accessed in the usual manner when implementing the kernel.
While this handles many such uses, however, we may need to determine
the sizes of the different data structures they use that should be allocated
by the host on the device (see \texttt{curand} examples in the \Rpkg{RCUDA}
documentation for more details).

\bibliography{gpu}


\end{document}


% Put this somewhere. Not certain where so added here.
In gputools, there is a routine named distance and another named
distanceLeaveOnGpu.  The two are similar. However, the latter is used
when we will utilize the distance results in another computation on
the GPU, e.g. clustering.  This is a good example of where it is more
convenient to be able to express and control these different
organizations and reorganizations of the primitive computations in a
high-level, interpreted language such as R.
Similarly, the code has to deal with the two cases
where we are computing all pairwise distances within a single data set
or between two different data sets. 
These computations are fixed in the C code and restrict how we can use them.

