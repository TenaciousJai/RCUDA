\section{Computing the Normal density}
Performance in \R{} puts a great premium on vectorized
operations.  Since most of the primitives are vectorized,
expressions that use these primitives are also vectorized. 
Since these primitives are vectorized in \C{} code,
they are fast.  Some of them also use multiple
processors when the vectors are sufficiently large
and there are additional available CPUs.

Element-wise vector operations in \R{} map naturally to GPUs.  We
perform the same computation on each element of the vector in a
separate thread on the GPU.  We define a kernel to operate on an
individual element and then we can apply that to each element of the
vector.

Consider the function \Rfunc{dnorm} in \R.
It computes the value for a Normal density
a vector of values, e.g.
\begin{RCode}
N = 1e6L
x = rnorm(N)
d = dnorm(x, mu, sigma)
\end{RCode}

We can define a similar kernel routine to compute
the density for a single observation.
The code will look similar to the following:
\begin{CCode}
extern "C"
__global__ void 
dnorm_kernel(float *vals, int N, float mu, float sigma)
{
    int myblock = blockIdx.x + blockIdx.y * gridDim.x;
    int blocksize = blockDim.x * blockDim.y * blockDim.z;
    int subthread = threadIdx.z*(blockDim.x * blockDim.y) +
                      threadIdx.y*blockDim.x + threadIdx.x;

    int idx = myblock * blocksize + subthread;
    float pi = 3.141592653589793;

    if(idx < N) {
       float std = (vals[idx] - mu)/sigma;
       float e = exp( - 0.5 * std * std);
       vals[idx] = e / ( sigma * sqrt(2 * pi));
    }
}
\end{CCode}
The initial expressions determine on which element of the vector this
particular thread should operate.  It does this by computing its
unique identifier in the grid of blocks of threads.  Once it has
computed the index of the element, it verifies that this is not a
redundant thread but is operating on a value within the extent of the
vector.  Then it computes the result.

In our kernel, we have passed the vector of values via the first
parameter.  We have arranged for the kernels to write their results
back into this vector since we don't expect to reuse this veector on
the device. This removes the need to have two related vectors for the
inputs and outputs in memory on the device simultaneously.

We compile this code,  either on the command line outside 
of \R{} or with 
\begin{RCode}
ptx = nvcc('dnorm.cu')
\end{RCode}
We can then load the resulting PTX file with
\begin{RCode}
mod = loadModule(ptx)
\end{RCode}

Now that we have the kernel, we can obtain
a reference to the kernel with
\begin{RCode}
k = mod$dnorm_kernel
\end{RCode}
%$
We can now apply this kernel to our vector above with
\begin{RCode}
ans = .gpu(k, x, N, mu, sigma, gridBy = N)
\end{RCode}
The \Rfunc{.gpu} function recognizes which arguments
are local vectors (not scalars) and, by default, returns
the updated contents of these vector arguments.
Therefore, this call to \Rfunc{.gpu} returns the new
contents of \Rvar{x}.

Note that \Rvar{N} has to be an integer. If it is not, we must coerce
it to an integer in the call or before.  The types of each argument
must match the corresponding type of the kernel. 
%XXX Have to talk about the mapping done by RCUDA.


We also created a kernel that does not overwrite its inputs.
This has the signature
\begin{CCode}
void dnorm_kernel(float *vals, int N, float mu, float sigma, float *out)
\end{CCode}
where the \Cparam{out} is the vector into which each kernel instance
stores its result.
We would invoke this in a similar manner as the previous kernel,
but here we have to pass this extra argument.
We can just allocate this output vector with a call to \Rfunc{numeric} 
and pass it in the call to \code{.gpu}.
The command
\begin{RCode}
ans = .gpu(k, x, N, mu, sigma, out = numeric(N), gridBy = N)
\end{RCode}
returns the update values of both vectors in the call,
i.e. \Rvar{x} and \Rarg{out}.
This involves more computation and memory
than is necessary.  We don't need the contents of
\Rvar{x}.
We use the \Rarg{outputs} parameter to specify which 
vectors to explicitly transfer. We can use the
name of the argument or its position (in the collection of kernel
arguments). For example, to return only the vector associated
with the variable named \Rarg{out}, we use
\begin{RCode}
ans = .gpu(k, x, N, mu, sigma, out = numeric(N),
            outputs = 'out', gridBy = N)
\end{RCode}
Note that this is different  from
\begin{RCode}
ans = .gpu(k, x, N, mu, sigma, out = numeric(N),
           gridBy = N)$out
\end{RCode}
%$
as we might use in \R's \Rfunc{.C} interface.
The reason this is different is that \Rfunc{.gpu} will
transfer and return both \Rvar{x} and \Rvar{out} and then 
we are extracting only the \Rvar{out} element.
By explicitly specifying which objects to transfer in
the call to \Rfunc{.gpu}, we avoid the overhead.



% The performance on the Quadro 600 GPUs does not 
% outperform R's own dnorm().
% See tm.Quadro
