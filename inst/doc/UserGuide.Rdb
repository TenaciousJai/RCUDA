<?xml version="1.0"?>
<article xmlns:r="http://www.r-project.org"
	 xmlns:xi="http://www.w3.org/2003/XInclude">

<articleinfo>

<title></title>

<author><firstname>Duncan</firstname><surname>Temple Lang</surname>
  <affiliation><orgname>University of California at Davis</orgname>
               <orgdiv>Department of Statistics</orgdiv>
  </affiliation>
</author>
</articleinfo>

<section>
<title></title>

<para>
The package provides the basics for working with CUDA from within
<r/>.  Soon we will have complete bindings to all of the routines in the CUDA SDK.
</para>
<para>
The basic concepts are described in the SDK documentation and we
provide a slightly more <r/>-like interface to them.  We can query the
available devices on the machine and get their properties.  We can
load a module containing PTX code (or other compatible formats).  We
can access the routines in a module and then invoke with the
<r:func>.cuda</r:func> function in <r/>.
We can pass scalar values from <r/> in calls to CUDA kernels,
and we can also pass pointers to memory allocated on the CUDA
device. We can copy data backwards and forwards between
the host and device.
</para>


<para>
We can explicitly allocate memory on the GPU with <r:func>cudaMalloc</r:func>
(or its alias <r:func>cudaAlloc</r:func>).
We can then copy data from <r/> to this space via <r:func>copyToDevice</r:func>.
We can combine the two steps - allocating and populating -
with <r:func>copyToDevice</r:func>:
<r:code>
x = rnorm(1e5L)
x.cu = copyToDevice(x)
</r:code>
The result is a reference to the GPU memory, along 
with information about its type and the number of elements.
This allows <r/> to take care of details for us such as
retrieving the values with 
<r:code>
vals = x.cu[]
</r:code>
This copies the contents of the GPU memory back to the host and into an <r/>
vector.
We can also assign values from <r/> to the 
GPU memory, e.g.,
<r:code>
x.cu[] = rnorm(1e5L)
</r:code>
</para>
<para>
We currently don't support assigning to specific subsets or individual
elements, but we can. Speed is a concern for these situations.
</para>


</section>
<section>
<title>Profiling</title>

<para>
We can use the CUDA profiling tools to gather information about what
parts of our code take how much time.  We can do this atomically on an
entire task (or sequence of expressions) or we can interleave calls to
start and stop the profiler to monitor or ignore parts of our
computations.  The <r:func>profileCUDA</r:func> function is the atomic
version. We give it the R expression we want to evaluate, presumably
involving some interaction with CUDA, and any other information and it
initializes and starts the profiler, runs the code, stops the profiler
and collects the results for us as a data frame.
<r:code>
<xi:include href="../../tests/profiler.R" parse="text"/>
</r:code>
</para>
<para>
We can specify a configuration file for the profiler,
either directly via the <r:arg>config</r:arg> argument or 
indirectly by setting the <r/> option <r:opt>CUDAProfilerConfig</r:opt>.
This 
<xi:include href="../../tests/prof.conf" parse="text"/>
</para>

<para>
We can also perform the steps listed above in separate 
<r/> calls.
<r:func>cudaProfiler</r:func> initializes
the profiler and we can specify the output file,
a configuration file and the desired format of 
the output - either CSV or key=value pairs.
Then we use <r:func>cudaStartProfiler</r:func>
and <r:func>cudaStopProfiler</r:func> to toggle the profiler's activities.
<r:code eval="false">
fname = cudaProfiler()
cudaStartProfiler()
 ....
cudaStopProfiler()
</r:code>
We can restart the profiler later 
and the turn it off again.

</para>


</section>
</article>