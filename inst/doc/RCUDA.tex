\documentclass[article]{jss}
\usepackage{comment}
\usepackage{graphicx}
%\usepackage{tikz}
%\usetikzlibrary{shadows,trees}
\usepackage{fancyvrb}
\usepackage{cprotect}

%\usepackage[firstpage]{draftwatermark}
%\SetWatermarkLightness{.95}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Paul Baines\\University of California at Davis\\
        Duncan Temple Lang\\University of California at Davis}
\title{RCUDA: General programming facilities for GPUs in R}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Paul Baines, Duncan Temple Lang}
\Plaintitle{RCUDA: General programming facilities for GPUs in R}
\Shorttitle{RCUDA}

%% an abstract and keywords
\Abstract{ General Purpose Graphical Programming Units (GPGPUs)
  provide the ability to perform computations in a massively parallel
  manner.  Their potential to significantly speed computations for
  certain classes of problems has been clearly demonstrated.  We
  describe an \proglang{R} package that provides high-level
  functionality that allows \proglang{R} programmers to experiment
  with and exploit NVIDIA GPUs.  The package provides an interface to
  the routines and data structures in the CUDA (Compute Unified Device
  Architecture) software development kit (SDK), and also provides
  higher-level \proglang{R} interfaces to these. Currently,
  programmers write the code that runs on the GPU in \proglang{C} code
  but call that code and manage the inputs and outputs in \proglang{R}
  code. We describe experimental approaches to compile \proglang{R}
  directly so that all computations can be expressed in \proglang{R}.
} \Keywords{GPUs, parallel computing, \R, \Rpkg{RCUDA} package}
\Plainkeywords{GPUs, parallel computing, R, RCUDA} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Paul Baines\\
  4210 Math Sciences Building, \\
  University of California at Davis \\
  One Shields Avenue\\
  Davis, CA 95616\\
  E-mail: \email{pbaines@ucdavis.edu}\\
  URL: \url{http://www.stat.ucdavis.edu/~pdbaines/}
\\
\\
  Duncan Temple Lang\\
  4210 Math Sciences Building, \\
  University of California at Davis \\
  One Shields Avenue\\
  Davis, CA 95616\\
  E-mail: \email{duncan@r-project.org}\\
  URL: \url{http://www.omegahat.org}
}
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\usepackage[strings]{underscore}
%\usepackage[T1]{fontenc}
%\catcode`\_=12

%\usepackage{newtxtext,newtxmath}

\def\CUDA{CUDA}
\def\C{\proglang{C}}
\def\R{\proglang{R}}
\def\llvm{\proglang{LLVM}}
\def\Rpkg#1{\pkg{#1}}
\def\Rfunc#1{\textsl{#1()}}
\def\Rop#1{\texttt{#1}}
\def\Rvar#1{\textsl{#1}}
\def\Cfunc#1{\textit{#1()}}
\def\Cvar#1{\textit{#1}}
\def\Cparam#1{\textsl{#1}}
\def\Cnull{\textsl{NULL}}
\def\Rnull{\texttt{NULL}}
\def\file#1{\textbf{#1}}
\def\Ctype#1{\texttt{#1}}
\def\Rclass#1{\textit{#1}}
\def\Rarg#1{\textbf{#1}}
\def\Roption#1{\dquote{\textsl{#1}}}

\def\Rexpr#1{\texttt{#1}}

\def\dquote#1{``#1''}

\def\ShFlag#1{\textit{#1}}

\DefineVerbatimEnvironment{RCode}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CCode}{Verbatim}{fontshape=tt}
\DefineVerbatimEnvironment{ShCode}{Verbatim}{fontshape=it}
\DefineVerbatimEnvironment{ROutput}{Verbatim}{fontshape=bf}

\begin{document}

\section{Introduction}\label{sec:Introduction}
GPUs have emerged as a computational platform for massively parallel
computations. Unlike CPUs, their performance continues to grow
dramatically.  They are very good for particular classes of
computations, some quite common in statistics.  It is important for
statisticians to be able to leverage these so that we have another
approach to make computations faster and so scale to larger data sets
or simulations.

% Compute Unified Device Architecture


GPUs require the programmer to use a different computational model
when implementing an algorithm.  They also use a slightly extended
version of the  \C{}  programming language and require
the programmer to think about two processing units - the host CPU
and the  GPU  device. The programmer must allocate and move data from the
host to the device and back.  This consumes resources.


The \pkg{RCUDA} package is quite different in intent and functionality
than other GPU-related \R packages such as
\Rpkg{gputools}~\cite{bib:gputools} and \Rpkg{rgpu}~\cite{bib:rgpu}.
\Rpkg{gputools} implements several commonly used statistical
algorithms in \C{} code that runs on a GPU and provides \R{} functions
to invoke those with data in \R.  The set of functions is fixed and
\R{} programmers wanting to implement a different algorithm or
approach for one of these algorithms must program in \C.

The \Rpkg{rgpu} package also provides implementation of a few
algorithms written in \C{} that run on a GPU.  However, it also
provides an ``interpreter'' for \R{} scalar mathematical expressions.
This does not appear to handle arbitrary code % in spite of its claims
and also has to map each expression from \R{} to a different
representation for each call from \R{} to the GPU.
It also interprets this on the GPU rather than using native code. 
% It  also doesn't install cleanly.  
We discuss in section~\ref{sec:LLVM} how might be able to compile a
larger subset of the \R{} language to native GPU code.

The \Rpkg{RCUDA} package is similar in nature to
\Rpkg{OpenCL}~\cite{bib:ROpenCl}.  It targets the CUDA SDK and NVIDIA
GPUs. This is based on the belief that the dedicated SDK can
outperform the more general OpenCL~\cite{bib:OpenCL} SDK.  Also,
\Rpkg{RCUDA} aims to expose the entire SDK to \R{} programmers. The
\Rpkg{OpenCL} package provides the essential functionality to invoke
kernels on the GPU, passing data from \R{} to the GPU and back.  This
difference illustrates one of the primary motivations of the
\Rpkg{RCUDA} package. We want \R{} programmers to experiment with
different features of the SDK and to explore the performance
characteristics of various programming strategies for GPUs.  Different
GPUs exhibit quite different performance characteristics, and
different programming paradigms and even tuning parameters also can
have significant impact. For this reason, we want to be able to be
able to control these aspects dynamically in a high-level language 
rather than fix them statically in a language such as \C.

%Norm Matloff's thrust package.



\section[The Basics of the RCUDA Package]{The Basics of the \Rpkg{RCUDA} package}\label{sec:}
In this section, we describe both the essential concepts of the CUDA
SDK and the \R{} interface to it provided by the \Rpkg{RCUDA} package.

At its very simplest, computing with GPUs involves 
\begin{enumerate}
\item writing (and compiling) a kernel that performs the computations,
\item allocating memory on the GPU device, 
\item copying data from the host to the device
\item invoking the kernel
\item copying the results from the device to the host.
\end{enumerate}
We will discuss each of these steps.

For now, we require that the kernel be compiled outside of \R.  This
means that an \R{} programmer can write and compile a kernel or
perhaps that it be compiled by somebody else and made available to the
\R{} programmer.  The \Rpkg{RCUDA} package can compile a kernel for
the \R{} user but it is often more convenient to use the command-line
to compile the code directly. 
We'll use the \Cfunc{dnorm} kernel as an example.
Our kernel code is reasonably simple for a CUDA kernel and is shown in
figure~\ref{fig:dnormKernelCode}.
 % should we show the code for the kernel?
% This probably requires a separate section  describing the basic idea
% of kernels and how they  know what to operate on and where to put
% their results. threadIdx, blockIdx, blockDim, gridDim. This requires
% some explanation,
\begin{figure}
\begin{CCode}
extern "C"
__global__ void dnorm_kernel(float *vals, int N, float mu, float sigma)
{
       // Taken from geco.mines.edu/workshop/aug2010/slides/fri/cuda1.pd
    int myblock = blockIdx.x + blockIdx.y * gridDim.x;
           /* how big is each block within a grid */
    int blocksize = blockDim.x * blockDim.y * blockDim.z;
            /* get thread within a block */
    int subthread = threadIdx.z*(blockDim.x * blockDim.y) + 
                         threadIdx.y*blockDim.x + threadIdx.x;

    int idx = myblock * blocksize + subthread;

	if(idx < N) {
            float std = (vals[idx] - mu)/sigma;
	    float e = exp( - 0.5 * std * std);
	    vals[idx] = e / ( sigma * sqrt(2 * 3.141592653589793));
	}
}
\end{CCode}
\caption{The \C{} code defining a GPU kernel to compute the Normal
  density.  This kernel writes the results back into the input
  vector, overwriting its values.
}\label{fig:dnormKernelCode}
\end{figure}

Before we can invoke a kernel, we have to load it into the host
process, i.e. \R.  We do this by loading the compiled code as a
\textit{Module}.  We use the \R{} function \Rfunc{loadModule} to do
this. It can read the code in various formats - the PTX text format
and the two binary formats cubin and fatbin.

We compile it with 
\begin{ShCode}
nvcc --ptx -o dnorm.ptx dnorm.cu
\end{ShCode}
Alternatively, we can compile the two binary formats with 
\begin{ShCode}
nvcc -cubin -m64 -gencode arch=compute_10,code=sm_10 -o dnorm.cubin dnorm.cu
nvcc -fatbin -m64 -gencode arch=compute_10,code=sm_10 -o dnorm.fatbin dnorm.cu
\end{ShCode}
respectively.
The \ShFlag{arch} flag specifies the GPU compatability level.%! Fix this

Once we have compiled the code, we can load it with
\begin{RCode}
filename = system.file("sampleKernels", "dnorm.ptx", package = "RCUDA")
mod = loadModule(filename)
\end{RCode}
Next, we can obtain a reference to the particular routine in the
module  that we want to invoke.  The \R{} interface makes the module
appear like a list and so we can use
\begin{RCode}
kernel = mod$dnorm_kernel
\end{RCode}
%$ - fool latex mode
to get this reference.

We now have the kernel, but need the data to pass to it.  We'll use a simple example
and simulate it in \R{} via the \Rfunc{rnorm} function.
\begin{RCode}
N = 1e6
mean = 2.3
sd =  2.1
x = rnorm(N, mean, sd)
\end{RCode}

We have outlined the explicit steps involved in calling a kernel.
However, we can shortcut these steps using the \R{} interface.  The
\Rfunc{.cuda} (or the synonomous \Rfunc{.gpu}) will copy an \R{} vector
argument to the device. So we can invoke the kernel with
\begin{RCode}
ans = .cuda(kernel, x, N, mean, sd, gridDim = c(62, 32), blockDim = 512)
\end{RCode}
We must explicitly specify the dimension for the grid and block to be
used when invoking the kernel. Each of these can specify
three-dimensions, however, any omitted dimension defaults to $1$.
We have chose $62$, $32$ and $512$ for this example as the product
of these exceeds the number of elements \Rvar{N}. We will have some
redundant threads on the GPU, but that is cheap.

The \Rfunc{.cuda} function recognizes the inputs and determines which
to copy to the device and which can be passed directly (i.e. the
scalar values \Rvar{N}, \Rvar{mean} and \Rvar{sd}). We'll see later
that we can pass arguments that refer to data or memory already on the
GPU and \Rfunc{.cuda} recognizes that it doesn't need to transfer
this, but merely pass the reference as-is.

The \Rfunc{.cuda} function also recognizes that since the data was
passed from \R{}, it may be modified and returns that vector.  If
there are multiple vector inputs, it returns all of these as list in
the same way the \Rfunc{.C} function behaves. However, \Rfunc{.cuda}
recognizes if a single input argument is a vector and so \Rvar{ans} in
our example contains the actual vector of normal density values.  More
generally, we can specify which inputs are to be copied back to \R{}
from the device via the \Rarg{outputs} parameter of the \Rfunc{.cuda}
function.

\Rfunc{.cuda} executs the kernel calls using the current context which
we describe in section~\ref{sec:Contexts}.  This same context must be
used to both load the module and call any of its kernels.

\subsection{Allocating Memory on the Device}
While the \Rfunc{.cuda} function processes (most) \R{} vectors for us,
we often want to explicitly control how values are passed from the
host (CPU) to the device (GPU). The \Rpkg{RCUDA} package provides
functions to control this and also some short-hand, covenient
mechanisms for implementing the transfers.  The two fundamental
functions are \Rfunc{copToDevice} and \Rfunc{copyFromDevice}.  These
are reasonably flexible functions.  \Rfunc{copyToDevice} takes an \R{}
object and copies its contents to memory on the GPU device.  By
default, it allocates the space on the device, using information about
the number of elements and the type of each element in the \R{} object
to determine the space needed.  For example, we can copy an \R{}
vector to the GPU in the following manner:
\begin{RCode}
dev.ptr = copyToDevice(x)
\end{RCode}

We can also explicitly allocate space on the device and then copy
values to that space.  The function \Rfunc{cudaMalloc} (and its alias
\Rfunc{cudaAlloc}) allocates space on the device.  We pass it the
number of elements and either the size of each element (in bytes) or
the name of the element type which it looks up to determine the number
of bytes for each element.  We can explicitly allocate memory and pass
this as the destination target for \Rfunc{copyToDevice}, e.g.
\begin{RCode}
ptr = cudaAlloc(N, elType = "numeric")
copyToDevice(x, ptr)
\end{RCode}

Why would we want to explicitly allocate space on the device rather
then letting \R{} copy vectors for us?  We may want to allocate the
space once and then reuse it to store different values at different
times. For example, consider a simple parametric bootstrap.  We
generate many samples each of the same length and we can reuse the
same memory.  We allocate the space once and then copy data to it
in each iteration and then use it via a call to a GPU kernel, e.g.,
\begin{RCode}
ptr = cudaMalloc(length(x), elType = class(x))
replicate(B, {
      copyToDevice(sample(x), ptr)
      .cuda(kernel, ptr, N, mean, sd, gridDim = c(62, 32), blockDim = 512))
})
\end{RCode}

By default, \Rfunc{copyToDevice} uses \Rfunc{cudaMalloc} to allocate
the space on the device.  \Rfunc{cudaMalloc} returns an object that
points to the allocated memory, but also contains the number and the
type of the elements, if specified.  This allows us to retrieve the
contents of the memory on the device.  The \Rfunc{copyFromDevice}
function does this.  It takes the pointer, the number of elements and
the type of each element.
We can specify these explicitly, but in many cases, we can use the 
more convenient  subset operator
\begin{RCode}
p[]
\end{RCode}
This utilizes the information stored when we allocated the space and
is equivalent to 
\begin{RCode}
copyFromDevice(p, p@nels, p@elTypeName)
\end{RCode}

We can also use the subset syntax to assign values to an existing
memory location on the device. 
\begin{RCode}
p[] <- rnorm(N)
\end{RCode}
copies the the values on the right-hand side.

We can use \Rfunc{cudaMalloc} to allocate space for arbitrary data
types as we only need the size of each element.  For known data types
such as \Ctype{float} or \Ctype{int} values, \Rpkg{RCUDA} knows how to
copy data both to and from the device.  This does not work for
arbitrary data types as we don't know how to copy each element.


When we no longer have an \R{} reference memory on the device (i.e. in
an \R{} variable), \R{} releases the memory using a finalizer routine
we register when allocating the space.  This allows \R{} programmers
to ignore memory management issues and treat the memory on the device
as a regular \R{} object.


%Pinned memory and other functions in the API.

\subsection{Contexts}
We can call many of the functions in the \Rpkg{RCUDA}  package
without having to know about CUDA context objects.
However, they are important for some computations.

Most \Rpkg{RCUDA} functions require an active context.  We can
explicitly create a default context using \Rfunc{createContext} or we
can use \Rfunc{cuGetContext} to query the current context and create
one if there is none.  When creating a context, we can specify the
device with which it is associated and also specify different flags or
options to control how it behaves.  These options are a combination of
individual options which we can specify in different ways. 
We can use \R{} variables representing the different options, e.g.
\Rvar{CU_CTX\_SCHED\_AUTO} and \Rvar{CU\_CTX\_MAP\_HOST},
and then we can combine them with the \Rop{|} operator. 
Alternatively, we can use a vector of names that identify the
different options. 
The following are equivalent
\begin{RCode}
c("SCHED_AUTO", "BLOCKING_SYNC", "MAP_HOST")
c("CU_CTX_SCHED_AUTO", "CU_CTX_BLOCKING_SYNC", "CU_CTX_MAP_HOST")
CU_CTX_SCHED_AUTO | CU_CTX_BLOCKING_SYNC | CU_CTX_MAP_HOST
\end{RCode}
The first avoids the \texttt{"CU_CTX"} prefix.

There are also functions that can query and set attributes of a
context such as the stack and heap size, shared memory configuration
and cache configuration.

 CUDA maintains a stack of contexts (per host thread, of which there
 is only one in \R).  When we create a context, it becomes the active
 one and is used in subsequent computations.  We can also explicitly
 push an existing context on to the top of the stack with
 \Rfunc{cuCtxPushCurrent}.  We can pop the current context off the top
 of the stack with \Rfunc{cuCtxPopCurrent}.  At present, \R{} users
 must explicitly release a context with \Rfunc{cuCtxtDestroy}.  It
 would be preferable to release the memory using \R's finalizer
 mechanism. However, this is, at best, complex because it is difficult
 to determine if CUDA is still using the context.



\subsection{Querying Devices}
The function \Rfunc{getNumDevices} tells us the number of devices on
the local machine.  
We can get the name of a device with \Rfunc{cuDeviceGetName}, e.g.
\begin{RCode}
cuDeviceGetName(1L)
\end{RCode}
\begin{ROutput}
[1] "GeForce GT 330M"
\end{ROutput}
We can query the characteristics of a device using
the \Rfunc{getDeviceProperties} function or
\Rfunc{cuDeviceGetAttributes}.  The former uses a deprecated routine
in the CUDA API, while the second queries all of the individual
attributes.  The queryable attributes include the maximum dimensions
of a grid and a block, the maximum number of threads per block and per
processor, the warp size, the number of multi-processors, shared
memory per block.  The names of the entire set of attributes are
available  via the \Rvar{CUdevice\_attributeValues} variable in the package.
Rather than querying all of the attributes in one call,
we can retrieve a single attribute with \Rfunc{cuDeviceGetAttribute}.
We specify the attribute using the name or value of one 
of the elements in \Rvar{CUdevice\_attributeValues}.


In addition to querying information about a device,
we can query the version of the CUDA SDK with
\Rfunc{cudaVersion}, e.g.
\begin{RCode}
cudaVersion()
\end{RCode}
\begin{ROutput}
 driver runtime 
   5000    5000 
\end{ROutput}


We can determine the amount of memory a device has with the function
\Rfunc{cuMemInfo}. Unlike the other functions for querying a device,
this doesn't take a device as an argument. Instead, it uses the device
associated with the current context.  Thus we have to have created a
context before calling this function, either explicitly or implicitly.
The function reports the total memory on the device, the amount free
and the proportion free.


\subsection{Profiling}
The reason to use GPUs for scientific computing is to improve
performance. Copying data between the host and the device is an
overhead and different approaches in the kernel code and even the
dimensions of the grid and block for controlling the threads.  It is
useful to understand where the entire code spends time in order to
improve the performance by reducing these bottlenecks.  As in \R{}
itself, we can profile computations involving the GPU with several
routines in the CUDA API.

We can profile an entire sequence of \R{} expressions with the \R{}
function \Rfunc{profileCUDA}.  This takes one or more \R{} expressions
(enclosed within {} for more than one expression) and returns the
profiling information as a data frame.
The following allocates space and reuses it for a simple bootstrap
to compute the likelihood for each sample:  
\begin{RCode}
B = 100
prof = profileCUDA( {
  p = copyToDevice(x)
  replicate(B, {
      p[] = sample(x, N, replace = TRUE)
      prod(.cuda(kernel, p, N, mean, sd, outputs = 1,  
                   gridDim = c(62, 32), blockDim = 512))
  })
})
\end{RCode}
This returns a data frame with rows for each line of output generated
by the CUDA profiler.  Each row corresponds to a particular CUDA
routine, kernel routine or routine called by the kernel.  There are
typically multiple rows for the same routine corresponding to when the
routine was observed by the profiler.  The columns of the data frame
correspond to the \dquote{counters} we specified for the profiler to
record.  We specify this via a configuration file the profiler reads
when it is instantiated. The \Rpkg{RCUDA} package provides a default
confugration file and uses that if the caller does not specify one.
We can use a different configuration file either by explicitly passing
a file name in the call to \Rfunc{profileCUDA} via the \Rarg{config}
parameter, or globally via the \R{} option
\Roption{CUDAProfilerConfig}.


Rather than profiling an entire collection of \R{} expressions as a
single unit, we can create a profiler and start and stop it at
different times to collect information about particular \R{}
expressions.  The \Rfunc{cudaProfiler} function creates the profiler,
and optionally starts it.  We specify the name of a file to which the
profiler writes the information, and we can select either of two
formats - CSV or name=value pairs.  We start and stop the profiler
with \Rfunc{cudaStartProfiler} and \Rfunc{cudaStopProfiler}.  We can
read the CSV output from the profiler with \Rfunc{readCUDAProfile}.
\Rfunc{profileCUDA} is merely a wrapper that uses all of these
functions.

The \Rfunc{summary} method for the result of \Rfunc{profileCUDA} and
\Rfunc{readCUDAProfile} aggregates the rows in the data frame for each
routine and gives the total counts for each as a new data frame.

\subsection{Memory Management}



\subsection{Other Routines in the API}
% Briefly mention the general groupings of other routines, e.g. stream,
% event  and what we omitted

For the most part, there is an \R{} function corresponding 
to  each routine in the CUDA SDK  that can run on the host.

We have not created bindings to the routines associated with textures
and surfaces.  Should these be important, we can generate this code.
 


\input{Examples}



\section{Low-level interface and its implementation}
We initially implemented the bindings to copy data between the host
(CPU) and device (GPU) and the ability to launch a kernel to verify
that this approach would work and to deal with the marshalling of \R{}
objects to the kernel calls and vice-versa.  We also added some
additional bindings to query the available GPUs and their properties.
Subsequently, however, we programmatically generated the binding (\R{}
and \C) code and the enumerated constants and information about the
data structures (e.g. the size of each so we can use that when
allocating memory on the device).  

% Show an example of a binding and how it appears.
Next, we describe a reasonably simple example of a programmatically
generated binding to illustrate the nature of the interface and to
help mapping  from the CUDA documentation to the \R{} functions.
One function is \Rfunc{cuDeviceGetAttribute} and corresponds to the routine
with the same name in the CUDA API.
The routine is defined  in the header file \file{cuda.h} as
\begin{Code}
CUresult CUDAAPI 
cuDeviceGetAttribute(int *pi, CUdevice_attribute attrib, CUdevice dev);
\end{Code}
The second parameter identifies which attribute we are querying and
the third parameter identifies the device. The latter is an integer,
with $0$ corresponding to the first device, $1$ for the second and so
on.  The first parameter is a pointer to a single integer. This is how
the CUDA API returns the result of the query.  This is an output
variable. Accordingly, we do not need to include in the \R{} function
that calls the \C{} routine.

The CUDA routine returns a status value of type \Ctype{CUresult}.  We
check this to see if the call was successful. If it was, we return the
result stored in \Cvar{pi}; otherwise, we raise an error in \R, using
the particular value of the status type and the CUDA error message.

The \R{} function we generate to interface to the CUDA routine is
defined as
\begin{RCode}
cuDeviceGetAttribute <-
function( attrib , dev  )
{
   ans = .Call('R_auto_cuDeviceGetAttribute', 
                   as(attrib, 'CUdevice_attribute'), 
                   as(dev, 'CUdevice'))
   if(is(ans, 'CUresult') && ans != 0)
       raiseError(ans, 'R_auto_cuDeviceGetAttribute')
   ans
}
\end{RCode}
This makes the call to the wrapper routine
\Cfunc{R\_auto\_cuDeviceGetAttribute}, passing it the two \R{} values
identifying the attribute and device of interest.  An important step
here is that this \R{} function coerces the arguments it receives to
the correct types.  Here we have an opportunity to use $1$-based
counting familiar to \R{} users for specifying the device.  The
coercion from the device number to a \Rclass{CUdevice} object in \R{}
performs the subtraction to map to the $0$-based couting used by CUDA.
We manually defined the \Rclass{CUdevice} class and the coercion method.

The function checks the status of the result to see if it is an \R{}
object of class \Rclass{CUresult}. If it is and not $0$, the call
produced an error and we raise this in \R. By doing this in \R{}
rather than in \C{} code, it is easier to raise exceptions/conditions
with classes corresponding to the type of error. This allows
programmers to react to particular types of errors, e.g. out of memory
or invalid device, in different ways.

The final piece of the bindings is the \C{} wrapper routine
\Cfunc{R\_auto\_cuDeviceGetAttribute}. This is defined as 
\begin{CCode}
SEXP
R_auto_cuDeviceGetAttribute(SEXP r_attrib, SEXP r_dev)
{
    SEXP r_ans = R_NilValue;
    int pi;
    CUdevice_attribute attrib = (CUdevice_attribute) INTEGER(r_attrib)[0];
    CUdevice dev = INTEGER(r_dev)[0];
    CUresult ans;
    ans = cuDeviceGetAttribute(& pi,  attrib,  dev);
    if(ans)
       return(R_cudaErrorInfo(ans));
    r_ans = ScalarInteger(pi) ;
    return(r_ans);
}
\end{CCode}
The logic is quite straightforward and very similar to how we would
write this manually. We convert the \R-level arguments to their
equivalent \C{} types.  We create a local variable (\Cvar{pi}) used to
retrieve the actual result.  We call the CUDA routine and check the
status value.  If there is an error, we call a manually written
routine \Cfunc{R\_cudaErrorInfo} which collects information about the
CUDA error in the form of an \R{} object.  If the CUDA call was
successful, we return the value of the local variable \Cvar{pi}
converted to the approriate \R{} type.



We used \Rpkg{RCIndex}~\cite{bib:RCIndex} to both read the
declarations in the CUDA header files and to generate the bindings.
Programmatically generating the bindings reduces the number of simple
programming errors and also allows us to update the bindings when new
versions of the API and SDK are released.

The CUDA API uses an idiom for most of its routines.  A routine
returns an error status value, and if this indicates success, the
actual results are accessible via the pointers we passed to the
routine. In other words, the routines return their values via input
pointers.  We specialized the general binding generation mechanism in
\Rpkg{RCIndex} to exploit this idiom and be able to understand what
was essentially an output parameter passed as a pointer input and what
were actual pointer input parameters.  This code to generate the
CUDA-specific bindings is included in the package.

We also generated some of the documentation for the \R{} functions
programmatically.  The CUDA header files have documentation in
comments directly before each routine and data structure.  We used
\Rpkg{RCIndex} to extract these comments and then process the
different parts describing the purpose of the routine, its parameters
and return value.



\section[Compiling R code to native GPU code]{Compiling \R{} code to native GPU code}\label{sec:LLVM}
% Move into Future work if we don't get this running.
The following is not yet fully implemented, but is something 
we know to be possible and related to work we are pursuing in a
slightly related project.
\llvm{} stands for the Low Level Virtual Machine
and is a compiler toolkit which we can embed in \R. 
This allows us to dynamically generate machine code within an \R{}
session and invoke those newly created routines, passing them 
inputs from \R{} and obtaining the results in \R.
This is very powerful and allows us to ``compile around the \R{}
interpreter'' and gain significant speedups for some common \R{}
idioms~\cite{bib:StatSciLLVM}.

\llvm{} can generate machine code for the machine's CPU, but can also
create PTX code to run on an NVIDIA GPU.  We can use our \R{} compiler
code (\Rpkg{RLLVMCompile}), or a specialized version of it for
targetting GPUs, and have it generate PTX code for kernel routines.
We can then load these routines onto the GPU and invoke them as a
regular kernels.  This was done using \R{} as an example by
researchers in NVIDIA~\cite{bib:libNVVM} investigating dynamic, interpreted
languages and GPUs.

\section{Future Work}

Now that we have the general-purpose bindings and access to the CUDA
API, we, and hopefully others, will explore how to map statistical
algorithms onto the GPU to take advantage of their potential.  We plan
to explore different examples and approaches and understand their
characteristics.  

We also plan to explore the potential benefits of aspects of the API
such as pinned memory, streams for interleaving computations,
peer-to-peer communication between two or more GPUs.

As the need arises, we may generate bindings to other GPU-related
libraries and APIs. These include There is no need to have \R{}
bindings to interface with kernel-level libraries such as
Thrust~\cite{bib:Thrust} and parts of curand~\cite{bib:curand}.
However, we may need to determine the sizes of the different data
structures they use that should be allocated by the host on the
device.


\bibliography{gpu}


\end{document}


% Put this somewhere. Not certain where so added here.
In gputools, there is a routine named distance and another named
distanceLeaveOnGpu.  The two are similar. However, the latter is used
when we will utilize the distance results in another computation on
the GPU, e.g. clustering.  This is a good example of where it is more
convenient to be able to express and control these different
organizations and reorganizations of the primitive computations in a
high-level, interpreted language such as R.
Similarly, the code has to deal with the two cases
where we are computing all pairwise distances within a single data set
or between two different data sets. 
These computations are fixed in the C code and restrict how we can use them.

